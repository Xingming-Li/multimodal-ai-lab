{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5469dc4e-0e38-4076-8da9-8e7321e32517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, json, shutil\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f3c8f6-e4e9-4e02-b0ec-6ff8f5ac953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20749973cf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config & output directories\n",
    "OUT_ROOT = \"vla_benchmark_output\"\n",
    "DATA_DIR = os.path.join(OUT_ROOT, \"dataset\")\n",
    "IMG_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "ARTIFACTS_DIR = os.path.join(OUT_ROOT, \"artifacts\")\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 128\n",
    "ACTIONS = [\"turn_left\", \"turn_right\", \"go_straight\", \"stop\", \"slow_down\"]\n",
    "DATASET_SIZE = 1000\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9496f3c6-70b7-48f2-b7dd-5c64cc3c7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text templates (diverse)\n",
    "INSTR_TEMPLATES = {\n",
    "    \"turn_left\": [\"Turn left\", \"Take a left\", \"Left at the intersection\", \"Make a left turn soon\"],\n",
    "    \"turn_right\": [\"Turn right\", \"Take a right\", \"Right at the intersection\", \"Make a right turn soon\"],\n",
    "    \"go_straight\": [\"Go straight\", \"Keep going\", \"Continue forward\", \"Proceed straight ahead\"],\n",
    "    \"stop\": [\"Stop\", \"Halt\", \"Bring vehicle to a stop\", \"Completely stop the vehicle\"],\n",
    "    \"slow_down\": [\"Slow down\", \"Reduce speed\", \"Caution - slow down\", \"Decelerate\"]\n",
    "}\n",
    "\n",
    "# Additional language variations for contradictory instructions\n",
    "NEGATION_TEMPLATES = [\n",
    "    \"Ignore the sign, {}\",\n",
    "    \"Despite the visual cue, {}\",\n",
    "    \"Do not follow the sign, {}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e95a2bc-c06c-411c-a611-c4d7d2c48243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphics helper functions\n",
    "def random_color(minv=100, maxv=255):\n",
    "    return tuple(np.random.randint(minv, maxv, size=3).tolist())\n",
    "\n",
    "def make_background(w, h):\n",
    "    # Gradient + slight road texture\n",
    "    base = Image.new(\"RGB\", (w,h), random_color(160,220))\n",
    "    draw = ImageDraw.Draw(base)\n",
    "    for y in range(h):\n",
    "        c = int(160 + 60 * (y / h) + np.random.randint(-6,6))\n",
    "        draw.line([(0,y),(w,y)], fill=(c,c,c))\n",
    "    # Optional horizontal faint stripes to simulate road texture\n",
    "    if random.random() < 0.6:\n",
    "        for i in range(0, h, 12):\n",
    "            draw.line([(0, i+np.random.randint(-2,2)), (w, i+np.random.randint(-2,2))], fill=(200,200,200,20))\n",
    "    return base\n",
    "\n",
    "def add_gaussian_noise(img, sigma=6):\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    arr = np.clip(arr + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(arr)\n",
    "\n",
    "def random_affine(img):\n",
    "    # Apply small affine transform (shear/scale/translate)\n",
    "    w,h = img.size\n",
    "    max_shear = 0.15\n",
    "    max_translate = 0.08\n",
    "    sx = random.uniform(-max_shear, max_shear)\n",
    "    sy = random.uniform(-max_shear, max_shear)\n",
    "    tx = random.uniform(-max_translate, max_translate) * w\n",
    "    ty = random.uniform(-max_translate, max_translate) * h\n",
    "    # Matrix for PIL transform: (a, b, c, d, e, f)\n",
    "    a = 1 + sx\n",
    "    b = sy\n",
    "    c = tx\n",
    "    d = -sy\n",
    "    e = 1 + sx\n",
    "    f = ty\n",
    "    return img.transform(img.size, Image.AFFINE, (a,b,c,d,e,f), resample=Image.BICUBIC)\n",
    "\n",
    "# Draw arrow polygon (centered) and rotate/scale/paste\n",
    "def draw_arrow(img, angle_deg, color=(10,10,10), thickness=None, length=None, center_jitter=8):\n",
    "    w,h = img.size\n",
    "    cx = w//2 + random.randint(-center_jitter, center_jitter)\n",
    "    cy = h//2 + random.randint(-center_jitter, center_jitter)\n",
    "    length = length or random.uniform(30,60)\n",
    "    thickness = thickness or random.randint(5,12)\n",
    "    arrow = Image.new(\"RGBA\", img.size, (0,0,0,0))\n",
    "    d = ImageDraw.Draw(arrow)\n",
    "    # Draw shaft\n",
    "    rad = math.radians(angle_deg)\n",
    "    x2 = cx + length * math.cos(rad)\n",
    "    y2 = cy + length * math.sin(rad)\n",
    "    d.line([(cx,cy),(x2,y2)], fill=color+(255,), width=thickness)\n",
    "    # Arrow head\n",
    "    head_len = max(10, int(length * 0.25))\n",
    "    left = (x2 - head_len*math.cos(rad-0.6), y2 - head_len*math.sin(rad-0.6))\n",
    "    right = (x2 - head_len*math.cos(rad+0.6), y2 - head_len*math.sin(rad+0.6))\n",
    "    d.polygon([ (x2,y2), left, right ], fill=color+(255,))\n",
    "    arrow = arrow.rotate(random.uniform(-7,7), expand=False)\n",
    "    img.paste(arrow, (0,0), arrow)\n",
    "\n",
    "def draw_stop_sign(img):\n",
    "    w,h = img.size\n",
    "    cx = w//2 + random.randint(-10,10)\n",
    "    cy = h//2 + random.randint(-10,10)\n",
    "    r = random.randint(18,32)\n",
    "    sign = Image.new(\"RGBA\", img.size, (0,0,0,0))\n",
    "    d = ImageDraw.Draw(sign)\n",
    "    poly = []\n",
    "    rot = random.random()*2*math.pi\n",
    "    for i in range(8):\n",
    "        ang = 2*math.pi*(i/8) + rot\n",
    "        x = cx + r*math.cos(ang)\n",
    "        y = cy + r*math.sin(ang)\n",
    "        poly.append((x,y))\n",
    "    d.polygon(poly, fill=(180+random.randint(-30,30), 0, 0, 255))\n",
    "    # STOP text\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", random.randint(12,18))\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    bbox = font.getbbox(\"STOP\")\n",
    "    tw, th = bbox[2]-bbox[0], bbox[3]-bbox[1]\n",
    "    d.text((cx-tw/2, cy-th/2), \"STOP\", fill=(255,255,255,255), font=font)\n",
    "    sign = sign.rotate(random.uniform(-25,25), expand=False)\n",
    "    img.paste(sign, (0,0), sign)\n",
    "\n",
    "def draw_obstacle(img):\n",
    "    w,h = img.size\n",
    "    cx = w//2 + random.randint(-18,18)\n",
    "    cy = h//2 + random.randint(-18,18)\n",
    "    r = random.randint(10,28)\n",
    "    obs = Image.new(\"RGBA\", img.size, (0,0,0,0))\n",
    "    d = ImageDraw.Draw(obs)\n",
    "    color = (90+random.randint(-25,25), 90+random.randint(-25,25), 90+random.randint(-25,25), 255)\n",
    "    d.ellipse([(cx-r,cy-r),(cx+r,cy+r)], fill=color)\n",
    "    # Optional stripes\n",
    "    if random.random() < 0.5:\n",
    "        d.line([(cx-r, cy), (cx+r,cy)], width=max(1, r//6), fill=(60,60,60,255))\n",
    "    obs = obs.rotate(random.uniform(-10,10), expand=False)\n",
    "    img.paste(obs, (0,0), obs)\n",
    "\n",
    "def draw_random_distractors(img, n=2):\n",
    "    w,h = img.size\n",
    "    for _ in range(n):\n",
    "        typ = random.choice([\"circle\",\"tri\",\"rect\",\"small_arrow\"])\n",
    "        layer = Image.new(\"RGBA\", img.size, (0,0,0,0))\n",
    "        d = ImageDraw.Draw(layer)\n",
    "        cx = random.randint(10,w-10); cy = random.randint(10,h-10)\n",
    "        s = random.randint(6,24)\n",
    "        color = (random.randint(50,200), random.randint(50,200), random.randint(50,200), 200)\n",
    "        if typ == \"circle\":\n",
    "            d.ellipse([(cx-s,cy-s),(cx+s,cy+s)], fill=color)\n",
    "        elif typ == \"tri\":\n",
    "            d.polygon([(cx,cy-s),(cx-s,cy+s),(cx+s,cy+s)], fill=color)\n",
    "        elif typ == \"rect\":\n",
    "            d.rectangle([(cx-s,cy-s),(cx+s,cy+s)], fill=color)\n",
    "        elif typ == \"small_arrow\":\n",
    "            # Small arrow as distractor\n",
    "            draw_arrow(layer, random.choice([0,90,180,270]), color=(30,30,30), thickness=4, length=18)\n",
    "        img.paste(layer, (0,0), layer)\n",
    "\n",
    "def occlude_random(img):\n",
    "    w,h = img.size\n",
    "    if random.random() < 0.35:\n",
    "        layer = Image.new(\"RGBA\", img.size, (0,0,0,0))\n",
    "        d = ImageDraw.Draw(layer)\n",
    "        x1 = random.randint(0, w//2); y1 = random.randint(0, h//2)\n",
    "        x2 = random.randint(w//2, w); y2 = random.randint(h//2, h)\n",
    "        d.rectangle([(x1,y1),(x2,y2)], fill=(random.randint(100,200),)*3 + (255,))\n",
    "        img.paste(layer, (0,0), layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b39919-28ac-4edd-8eaa-0186a800bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic scene generator\n",
    "def generate_scene(action, make_contradictory=False):\n",
    "    img = make_background(IMG_SIZE, IMG_SIZE)\n",
    "    # Add lane markings sometimes\n",
    "    if random.random() < 0.5:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        for i in range(0, IMG_SIZE, 20):\n",
    "            x = IMG_SIZE//2 + random.randint(-6,6)\n",
    "            draw.rectangle([(x, i), (x+3, i+10)], fill=(230,230,230))\n",
    "    # Place primary object (will add distractors too)\n",
    "    if action == \"turn_left\":\n",
    "        draw_arrow(img, 180, color=(10,10,10))\n",
    "    elif action == \"turn_right\":\n",
    "        draw_arrow(img, 0, color=(10,10,10))\n",
    "    elif action == \"go_straight\":\n",
    "        draw_arrow(img, 270, color=(10,10,10))\n",
    "    elif action == \"stop\":\n",
    "        draw_stop_sign(img)\n",
    "    elif action == \"slow_down\":\n",
    "        draw_obstacle(img)\n",
    "    # Add distractors and multiple objects\n",
    "    if random.random() < 0.7:\n",
    "        draw_random_distractors(img, n=random.randint(0,2))\n",
    "    # Sometimes add an extra driving-related object (to make multi-object scenes)\n",
    "    if random.random() < 0.35:\n",
    "        extra = random.choice([\"stop\",\"turn_left\",\"turn_right\",\"slow_down\"])\n",
    "        if extra == \"stop\":\n",
    "            draw_stop_sign(img)\n",
    "        elif extra == \"turn_left\":\n",
    "            draw_arrow(img, 180)\n",
    "        elif extra == \"turn_right\":\n",
    "            draw_arrow(img, 0)\n",
    "        else:\n",
    "            draw_obstacle(img)\n",
    "    # Occlusion and noise\n",
    "    occlude_random(img)\n",
    "    if random.random() < 0.5:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0,1.8)))\n",
    "    img = add_gaussian_noise(img, sigma=random.randint(3,10))\n",
    "    # Random affine perspective\n",
    "    if random.random() < 0.5:\n",
    "        img = random_affine(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc32217f-5890-48fa-b3f6-59ea19f01902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset (this may take a moment)...\n",
      "Saved manifest with 1000 samples to vla_benchmark_output\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation with instruction logic\n",
    "def generate_dataset(n_samples=DATASET_SIZE, out_dir=IMG_DIR):\n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        # Create class\n",
    "        action = random.choice(ACTIONS)\n",
    "        # Decide whether this sample will be contradictory (language overrides vision)\n",
    "        is_contradictory = (random.random() < 0.05)  # ~5% contradictory\n",
    "        img = generate_scene(action)\n",
    "        # Choose instruction: sometimes contradictory, sometimes normal, sometimes more verbose\n",
    "        instr = random.choice(INSTR_TEMPLATES[action])\n",
    "        if random.random() < 0.15:\n",
    "            # Verbose paraphrase\n",
    "            instr = instr + \". \" + random.choice([\"Careful.\", \"Proceed with caution.\", \"Follow instruction.\"])\n",
    "        if is_contradictory:\n",
    "            # Pick a different ground-truth action that the instruction mandates\n",
    "            alt = random.choice([a for a in ACTIONS if a != action])\n",
    "            # Make instruction assert alt but maybe with negation flips\n",
    "            base = random.choice(INSTR_TEMPLATES[alt])\n",
    "            # Sometimes prepend negation to create complex language\n",
    "            if random.random() < 0.5:\n",
    "                instr = random.choice(NEGATION_TEMPLATES).format(base)\n",
    "            else:\n",
    "                instr = base\n",
    "            ground_truth_action = alt\n",
    "        else:\n",
    "            ground_truth_action = action\n",
    "        fname = f\"img_{i:05d}.png\"\n",
    "        path = os.path.join(out_dir, fname)\n",
    "        img.save(path)\n",
    "        samples.append({\"image\": fname, \"instruction\": instr, \"action\": ground_truth_action, \"orig_visual\": action, \"contradictory\": is_contradictory})\n",
    "    return samples\n",
    "\n",
    "print(\"Generating dataset (this may take a moment)...\")\n",
    "samples = generate_dataset()\n",
    "with open(os.path.join(DATA_DIR, \"manifest.json\"), \"w\") as f:\n",
    "    json.dump(samples, f, indent=2)\n",
    "print(\"Saved manifest with\", len(samples), \"samples to\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb90590-c17a-4619-b564-1f2eaef5069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class with preprocessing contained\n",
    "class VLADataset(Dataset):\n",
    "    def __init__(self, samples, img_dir, max_tokens=12):\n",
    "        self.samples = samples\n",
    "        self.img_dir = img_dir\n",
    "        self.max_tokens = max_tokens\n",
    "        self.vocab = self.build_vocab(samples)\n",
    "        \n",
    "    def build_vocab(self, samples):\n",
    "        vocab = {\"<pad>\":0, \"<unk>\":1}\n",
    "        idx = 2\n",
    "        for s in samples:\n",
    "            tokens = s[\"instruction\"].lower().replace(\",\", \"\").replace(\".\", \"\").split()\n",
    "            for t in tokens:\n",
    "                if t not in vocab:\n",
    "                    vocab[t] = idx; idx += 1\n",
    "        return vocab\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        toks = text.lower().replace(\",\", \"\").replace(\".\", \"\").split()\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<unk>\"]) for t in toks][:self.max_tokens]\n",
    "        if len(ids) < self.max_tokens:\n",
    "            ids += [self.vocab[\"<pad>\"]] * (self.max_tokens - len(ids))\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    \n",
    "    def preprocess_image(self, path):\n",
    "        img = Image.open(path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "        arr = np.array(img).astype(np.float32)/255.0\n",
    "        arr = (arr - 0.5)/0.5\n",
    "        return torch.tensor(arr).permute(2,0,1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        img_t = self.preprocess_image(os.path.join(self.img_dir, s[\"image\"]))\n",
    "        text_ids = self.encode_text(s[\"instruction\"])\n",
    "        label = ACTIONS.index(s[\"action\"])\n",
    "        return img_t, text_ids, label, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ccf07c-08a8-4fd7-ac89-a69f8ec96fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready. Vocab size: 44. Train/Val sizes: 800/200\n"
     ]
    }
   ],
   "source": [
    "# Create train & validation dataloaders\n",
    "dataset = VLADataset(samples, IMG_DIR)\n",
    "train_n = int(0.8 * len(dataset)); val_n = len(dataset) - train_n\n",
    "train_ds, val_ds = random_split(dataset, [train_n, val_n], generator=torch.Generator().manual_seed(SEED))\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"Dataset ready. Vocab size: {len(dataset.vocab)}. Train/Val sizes: {len(train_ds)}/{len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72442fec-1d1c-4c6a-8058-00c7156c9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models: small CNN, tiny text encoder, vision-only model, fused model\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=2, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(), nn.Linear(128, out_dim), nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TinyTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, max_tokens=12):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.max_tokens = max_tokens\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)  # (B, T, emb)\n",
    "        return e.mean(dim=1)  # (B, emb)\n",
    "\n",
    "class VisionOnlyModel(nn.Module):\n",
    "    def __init__(self, num_actions=len(ACTIONS)):\n",
    "        super().__init__()\n",
    "        self.vision = SmallCNN(out_dim=256)\n",
    "        self.classifier = nn.Sequential(nn.Linear(256,128), nn.ReLU(), nn.Linear(128, num_actions))\n",
    "    \n",
    "    def forward(self, img, text_ids=None):\n",
    "        v = self.vision(img)\n",
    "        return self.classifier(v)\n",
    "\n",
    "class FusedVLA(nn.Module):\n",
    "    def __init__(self, vocab_size, vision_dim=256, text_dim=128, num_actions=len(ACTIONS)):\n",
    "        super().__init__()\n",
    "        self.vision = SmallCNN(out_dim=vision_dim)\n",
    "        self.text = TinyTextEncoder(vocab_size, text_dim)\n",
    "        fusion_dim = vision_dim + text_dim\n",
    "        self.fuse = nn.Sequential(nn.Linear(fusion_dim, 512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                  nn.Linear(512,256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.2),\n",
    "                                  nn.Linear(256,128),\n",
    "                                  nn.ReLU())\n",
    "\n",
    "        self.classifier = nn.Linear(128, num_actions)\n",
    "    \n",
    "    def forward(self, img, text_ids):\n",
    "        v = self.vision(img)\n",
    "        t = self.text(text_ids)\n",
    "        x = torch.cat([v,t], dim=1)\n",
    "        x = self.fuse(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40bda698-27f2-4981-b1cb-6c024edcb728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Training & evaluation utilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=12, lr=1e-3, name=\"model\"):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "    for ep in range(1, epochs+1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        t_loss = 0.0; t_preds=[]; t_labels=[]\n",
    "        for imgs, txts, labels, _ in train_loader:\n",
    "            imgs = imgs.to(device); txts = txts.to(device); labels = labels.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(imgs, txts) if txts is not None else model(imgs)\n",
    "            loss = crit(logits, labels)\n",
    "            loss.backward(); opt.step()\n",
    "            t_loss += loss.item() * imgs.size(0)\n",
    "            t_preds += logits.argmax(dim=1).cpu().tolist()\n",
    "            t_labels += labels.cpu().tolist()\n",
    "        train_loss = t_loss / len(train_loader.dataset)\n",
    "        train_acc = accuracy_score(t_labels, t_preds)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        v_loss = 0.0; v_preds=[]; v_labels=[]\n",
    "        with torch.no_grad():\n",
    "            for imgs, txts, labels, _ in val_loader:\n",
    "                imgs = imgs.to(device); txts = txts.to(device); labels = labels.to(device)\n",
    "                logits = model(imgs, txts) if txts is not None else model(imgs)\n",
    "                loss = crit(logits, labels)\n",
    "                v_loss += loss.item() * imgs.size(0)\n",
    "                v_preds += logits.argmax(dim=1).cpu().tolist()\n",
    "                v_labels += labels.cpu().tolist()\n",
    "        val_loss = v_loss / len(val_loader.dataset)\n",
    "        val_acc = accuracy_score(v_labels, v_preds)\n",
    "        history[\"train_loss\"].append(train_loss); history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss); history[\"val_acc\"].append(val_acc)\n",
    "        print(f\"{name} Epoch {ep}/{epochs}  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "    # Save model\n",
    "    ckpt = os.path.join(ARTIFACTS_DIR, f\"{name}.pth\")\n",
    "    torch.save({\"state\":model.state_dict(), \"vocab\": dataset.vocab}, ckpt)\n",
    "    print(\"Saved\", ckpt)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_and_plot(model, loader, title_prefix=\"model_eval\", name=\"model\"):\n",
    "    model.eval()\n",
    "    preds=[]; labels=[]\n",
    "    samples_info=[]\n",
    "    with torch.no_grad():\n",
    "        for imgs, txts, lbls, meta in loader:\n",
    "            imgs = imgs.to(device); txts = txts.to(device); lbls = lbls.to(device)\n",
    "            logits = model(imgs, txts) if txts is not None else model(imgs)\n",
    "            preds += logits.argmax(dim=1).cpu().tolist()\n",
    "            labels += lbls.cpu().tolist()\n",
    "            samples_info += meta\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds, labels=list(range(len(ACTIONS))))\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.set_xticks(range(len(ACTIONS))); ax.set_yticks(range(len(ACTIONS)))\n",
    "    ax.set_xticklabels(ACTIONS, rotation=45, ha='right'); ax.set_yticklabels(ACTIONS)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j,i, int(cm[i,j]), ha='center', va='center', color='white' if cm[i,j] > cm.max()/2 else 'black')\n",
    "    plt.title(f\"{title_prefix} (acc={acc:.3f})\")\n",
    "    plt.colorbar(im)\n",
    "    out_png = os.path.join(ARTIFACTS_DIR, f\"{name}_confusion.png\")\n",
    "    plt.tight_layout(); plt.savefig(out_png); plt.close()\n",
    "    print(f\"Saved confusion matrix to {out_png} (acc={acc:.3f})\")\n",
    "    # Return raw metrics\n",
    "    return {\"acc\": acc, \"cm\": cm, \"preds\": preds, \"labels\": labels, \"samples_info\": samples_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e47717-bbfa-47ac-8299-206ba24e298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vision-only baseline...\n",
      "vision_only Epoch 1/12  train_loss=1.6024 train_acc=0.2462  val_loss=1.5727 val_acc=0.3000\n",
      "vision_only Epoch 2/12  train_loss=1.4980 train_acc=0.3500  val_loss=1.4786 val_acc=0.3300\n",
      "vision_only Epoch 3/12  train_loss=1.4344 train_acc=0.3300  val_loss=1.4346 val_acc=0.3300\n",
      "vision_only Epoch 4/12  train_loss=1.4084 train_acc=0.3625  val_loss=1.4218 val_acc=0.3400\n",
      "vision_only Epoch 5/12  train_loss=1.4064 train_acc=0.3650  val_loss=1.4421 val_acc=0.2800\n",
      "vision_only Epoch 6/12  train_loss=1.4133 train_acc=0.3225  val_loss=1.4321 val_acc=0.2950\n",
      "vision_only Epoch 7/12  train_loss=1.4087 train_acc=0.3762  val_loss=1.4375 val_acc=0.3350\n",
      "vision_only Epoch 8/12  train_loss=1.3999 train_acc=0.3475  val_loss=1.4506 val_acc=0.2550\n",
      "vision_only Epoch 9/12  train_loss=1.3932 train_acc=0.3700  val_loss=1.4258 val_acc=0.3350\n",
      "vision_only Epoch 10/12  train_loss=1.3887 train_acc=0.3713  val_loss=1.4329 val_acc=0.3750\n",
      "vision_only Epoch 11/12  train_loss=1.3718 train_acc=0.3937  val_loss=1.4183 val_acc=0.3050\n",
      "vision_only Epoch 12/12  train_loss=1.3556 train_acc=0.3900  val_loss=1.4186 val_acc=0.3550\n",
      "Saved vla_benchmark_output\\artifacts\\vision_only.pth\n",
      "\n",
      "Training fused VLA model...\n",
      "fused_vla Epoch 1/12  train_loss=1.5212 train_acc=0.4000  val_loss=1.1959 val_acc=0.8850\n",
      "fused_vla Epoch 2/12  train_loss=0.7796 train_acc=0.8525  val_loss=0.2547 val_acc=0.9850\n",
      "fused_vla Epoch 3/12  train_loss=0.1650 train_acc=0.9788  val_loss=0.0349 val_acc=1.0000\n",
      "fused_vla Epoch 4/12  train_loss=0.0192 train_acc=0.9988  val_loss=0.0021 val_acc=1.0000\n",
      "fused_vla Epoch 5/12  train_loss=0.0027 train_acc=1.0000  val_loss=0.0004 val_acc=1.0000\n",
      "fused_vla Epoch 6/12  train_loss=0.0009 train_acc=1.0000  val_loss=0.0002 val_acc=1.0000\n",
      "fused_vla Epoch 7/12  train_loss=0.0005 train_acc=1.0000  val_loss=0.0001 val_acc=1.0000\n",
      "fused_vla Epoch 8/12  train_loss=0.0005 train_acc=1.0000  val_loss=0.0001 val_acc=1.0000\n",
      "fused_vla Epoch 9/12  train_loss=0.0005 train_acc=1.0000  val_loss=0.0001 val_acc=1.0000\n",
      "fused_vla Epoch 10/12  train_loss=0.0004 train_acc=1.0000  val_loss=0.0001 val_acc=1.0000\n",
      "fused_vla Epoch 11/12  train_loss=0.0003 train_acc=1.0000  val_loss=0.0001 val_acc=1.0000\n",
      "fused_vla Epoch 12/12  train_loss=0.0003 train_acc=1.0000  val_loss=0.0001 val_acc=1.0000\n",
      "Saved vla_benchmark_output\\artifacts\\fused_vla.pth\n"
     ]
    }
   ],
   "source": [
    "# Train 2 models: vision-only & fused\n",
    "vmodel = VisionOnlyModel(num_actions=len(ACTIONS))\n",
    "fmodel = FusedVLA(vocab_size=len(dataset.vocab), vision_dim=256, text_dim=128, num_actions=len(ACTIONS))\n",
    "\n",
    "print(\"Training vision-only baseline...\")\n",
    "vmodel, vhist = train_model(vmodel, train_loader, val_loader, epochs=12, lr=1e-3, name=\"vision_only\")\n",
    "\n",
    "print(\"\\nTraining fused VLA model...\")\n",
    "fmodel, fhist = train_model(fmodel, train_loader, val_loader, epochs=12, lr=1e-3, name=\"fused_vla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b96be2-fad9-47d9-94c9-f28163bab74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated test dataset with 200 samples at vla_benchmark_output\\dataset\\test\n",
      "Dataset ready. Test size: 200\n"
     ]
    }
   ],
   "source": [
    "# # Create test dataloader\n",
    "def generate_test_dataset(n_samples=200, out_dir=os.path.join(DATA_DIR, \"test\")):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    test_samples = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        action = random.choice(ACTIONS)\n",
    "\n",
    "        # Test images can be slightly different distribution\n",
    "        img = generate_scene(action, make_contradictory=False)\n",
    "\n",
    "        instr = random.choice(INSTR_TEMPLATES[action])\n",
    "\n",
    "        fname = f\"test_{i:05d}.png\"\n",
    "        img.save(os.path.join(out_dir, fname))\n",
    "\n",
    "        test_samples.append({\n",
    "            \"image\": fname,\n",
    "            \"instruction\": instr,\n",
    "            \"action\": action\n",
    "        })\n",
    "    \n",
    "    manifest_path = os.path.join(DATA_DIR, \"manifest_test.json\")\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        json.dump(test_samples, f, indent=2)\n",
    "\n",
    "    print(f\"Generated test dataset with {len(test_samples)} samples at {out_dir}\")\n",
    "    return test_samples\n",
    "\n",
    "test_samples = generate_test_dataset()\n",
    "test_ds = VLADataset(test_samples, img_dir=os.path.join(DATA_DIR, \"test\"))\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "print(f\"Dataset ready. Test size: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655d6c2c-2c21-4367-9f76-77bcfb4a64c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models and saving confusion matrices...\n",
      "Saved confusion matrix to vla_benchmark_output\\artifacts\\vision_only_confusion.png (acc=0.380)\n",
      "Saved confusion matrix to vla_benchmark_output\\artifacts\\fused_vla_confusion.png (acc=0.410)\n",
      "Wrote note.json to vla_benchmark_output\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Evaluate & save plots\n",
    "print(\"Evaluating models and saving confusion matrices...\")\n",
    "ve = evaluate_and_plot(vmodel, test_loader, title_prefix=\"Vision-only eval\", name=\"vision_only\")\n",
    "fe = evaluate_and_plot(fmodel, test_loader, title_prefix=\"Fused VLA eval\", name=\"fused_vla\")\n",
    "\n",
    "# Save a short note/artifact description\n",
    "note = {\n",
    "    \"description\": \"VLA benchmark with synthetic but challenging dataset (distractors, contradictions, occlusions)\",\n",
    "    \"dataset_manifest\": os.path.join(DATA_DIR, \"manifest.json\"),\n",
    "    \"artifacts\": {\n",
    "        \"vision_only_checkpoint\": os.path.join(ARTIFACTS_DIR, \"vision_only.pth\"),\n",
    "        \"fused_vla_checkpoint\": os.path.join(ARTIFACTS_DIR, \"fused_vla.pth\"),\n",
    "        \"confusion_matrices\": [\n",
    "            os.path.join(ARTIFACTS_DIR, \"vision_only_confusion.png\"),\n",
    "            os.path.join(ARTIFACTS_DIR, \"fused_vla_confusion.png\")\n",
    "        ]\n",
    "    },\n",
    "    \"notes\": \"To upgrade, replace TinyTextEncoder and SmallCNN with pretrained models.\"\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS_DIR, \"note.json\"), \"w\") as f:\n",
    "    json.dump(note, f, indent=2)\n",
    "print(\"Wrote note.json to\", ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ff4738-273c-42bd-8c55-2d06ece3bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: vla_benchmark_output\n",
      "Dataset manifest: vla_benchmark_output\\dataset\\manifest.json\n",
      "Artifacts: vla_benchmark_output\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Where outputs are\n",
    "print(\"Outputs:\", OUT_ROOT)\n",
    "print(\"Dataset manifest:\", os.path.join(DATA_DIR, \"manifest.json\"))\n",
    "print(\"Artifacts:\", ARTIFACTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
